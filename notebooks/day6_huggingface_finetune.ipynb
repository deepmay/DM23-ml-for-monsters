{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc350c",
   "metadata": {},
   "source": [
    "# Fine-tuning a HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef571f",
   "metadata": {},
   "source": [
    "## Code Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9c9d5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8039c01",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "- A common use of LLMs is to leverage their **generalized** linguistic capacities by finetunint them for a **particular** task\n",
    "- For instance: We could take an LLM and train it to... \n",
    "    - classify text sequences\n",
    "    - classify tokens\n",
    "    - produce dialogue\n",
    "    - answer questions\n",
    "    - etc etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9c5e1",
   "metadata": {},
   "source": [
    "## Author Attribution\n",
    "\n",
    "- The task I want to train the model to perform on is to identify authors of text\n",
    "    - This is known as \"author attribution\"\n",
    "    - E.g. Italian Computer Scientists tried to identify Elena Ferrante by comparing her work with known Italian authors and journalists\n",
    "- We'll be using one of the few author attribution datasets on Huggingface \n",
    "    - Uses text from 13 journalists at the Guardian\n",
    "-  We can find the [data here](https://huggingface.co/datasets/guardian_authorship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ecc66",
   "metadata": {},
   "source": [
    "We load it by calling ```load_dataset```. The function needs the url of the dataset and a specification of which part of the data we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c161776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('guardian_authorship', 'cross_genre_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4854d0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 9,\n",
       " 'topic': 2,\n",
       " 'article': 'We had a traditional Christmas. The pilot light went out in the boiler. My mother was bad-tempered. The giblets made the dog sick. Sheffield Wednesday lost. But one unexpected incident shone more brightly than the star in the east. Princess Anne\\'s behaviour immediately after she had celebrated the birth of the Prince of Peace made it the most memorable season of goodwill for years. I offer her royal highness my humble congratulations and hope it is not lese-majeste on my part to add: \"Keep it up, Ma\\'am. Keep it up.\" Endpiece readers may not be familiar with what happened outside Sandringham Church - and may find the details of the story difficult to believe even when they hear them. But they were reported in the tabloid newspapers and are therefore beyond dispute. A 75-year-old lady called Mrs Halfpenny made a basket - plaiting the wicker with her own hands - and filled it with flowers. \\n',\n",
       " 'label': 12,\n",
       " '__index_level_0__': 95}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0444a",
   "metadata": {},
   "source": [
    "There are some issues with the data, so I wrote a quick script to fix it. \n",
    "- Merge train, test and validate as pandas df\n",
    "- Create new Dataset\n",
    "- Do my own train_test_split\n",
    "\n",
    "Most often this is **not** the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5eb57db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba0c603fee44f988302e33cd658206d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce27f4a14c343ecb033e91395167caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fix_guardian_data(dataset):\n",
    "    # Add a label column to the data\n",
    "    dataset['train'] = dataset['train'].add_column(\"label\", dataset['train']['author'])\n",
    "    dataset['test'] = dataset['test'].add_column(\"label\", dataset['test']['author'])\n",
    "    dataset['validation'] = dataset['validation'].add_column(\"label\", dataset['validation']['author'])\n",
    "    \n",
    "    # We want to do our own test-train split\n",
    "    # To do this, I first make the data into one big dataframe\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    all_data = pd.concat([train_df, test_df, val_df])\n",
    "    \n",
    "    # Now I create a Huggingface dataset from that dataframe\n",
    "    dataset = Dataset.from_pandas()\n",
    "    \n",
    "    # I decide which column is the 'label' column\n",
    "    dataset = dataset.class_encode_column(\"label\")\n",
    "    \n",
    "    # Then I take the train_test_split. I want 20% of the data to be in the test set\n",
    "    dataset = dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "    return dataset\n",
    "\n",
    "dataset = fix_guardian_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "981abe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'topic', 'article', 'label', '__index_level_0__'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['author', 'topic', 'article', 'label', '__index_level_0__'],\n",
       "        num_rows: 89\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e6702b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff51f1",
   "metadata": {},
   "source": [
    "Now we tokenize, as always for NLP. \n",
    "- Different LLM's use different tokenizers. \n",
    "- Like the model, our tokenizer needs to know where in the Huggingface Hub to look for specs to tokenize\n",
    "- We can use the  ```AutoTokenizer``` class instead of setting a particular tokenizer class\n",
    "\n",
    "We will be using DistilBERT, a smaller and more nimble version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "7b11a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d751c1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11faef075ce04c27a666f92b1108594f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f692a53c6e402f8d1a4e61cd9999c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"article\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564a2d0",
   "metadata": {},
   "source": [
    "Next we set our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "12c022bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd64fbc",
   "metadata": {},
   "source": [
    "We feed most hyperparameters to the the [```TrainingArguments``` class](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "684a86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "716c6bf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (624830870.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [260]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Now we can specify our model. We'll be using\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "Now we can specify our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "603627a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254a6c6",
   "metadata": {},
   "source": [
    "When we train, we want to keep track of the model performance. For this we need to give the model a fucntion that takes in the eval and returns some sort of ... . For this we can use the ```evaluate``` library and write a function around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dd056a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b820e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7853ea",
   "metadata": {},
   "source": [
    "We can also create so called \"callbacks\". \n",
    "- These are objects that customize the training loop\n",
    "- Some of the deftault ones have [their own classes in HuggingFace](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/callback)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbb5df",
   "metadata": {},
   "source": [
    "In our case, we want the model to stop if it didn't improve during 3 sequential epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6e95d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde2f0f",
   "metadata": {},
   "source": [
    "Finally, we create a [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). \n",
    "- This is a class HuggingFace inherits from [```PyTorch Lightning```](https://lightning.ai/docs/pytorch/stable/common/trainer.html)\n",
    "- Used in many other libraries, like TorchGeo\n",
    "- Given an instance of a model class, this does the whole job of forward and backward passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f44a6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42),\n",
    "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff999ee",
   "metadata": {},
   "source": [
    "Now we just run ```train()```, like with ```PyTorch```!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c999cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/dm23/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 1:02:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.535404</td>\n",
       "      <td>0.168539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.484488</td>\n",
       "      <td>0.202247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.382490</td>\n",
       "      <td>0.348315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.289948</td>\n",
       "      <td>0.393258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.209553</td>\n",
       "      <td>0.382022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.156970</td>\n",
       "      <td>0.393258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.056545</td>\n",
       "      <td>0.505618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.997387</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.975646</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.968134</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=2.111922064887153, metrics={'train_runtime': 3744.6896, 'train_samples_per_second': 0.948, 'train_steps_per_second': 0.12, 'total_flos': 470351515699200.0, 'train_loss': 2.111922064887153, 'epoch': 10.0})"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb4a00",
   "metadata": {},
   "source": [
    "The model has finished training! \n",
    "- Now we can use it in a ```Huggingface``` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ae339184",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a3616",
   "metadata": {},
   "source": [
    "The model outputs probabilities, no need to work with logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a66b1018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_3', 'score': 0.1491585373878479}]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(tokenized_datasets[\"test\"][50]['article'][:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcf2fa",
   "metadata": {},
   "source": [
    "Now we'll compare the model predictions on the test set with our predictions on it.\n",
    "- We'll check if ```pred_lab == real_lab``` and count how many times itÂ´s ```True```.\n",
    "    - This is our count for how many times we predicted correctly :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bb18a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for idx in range(len(tokenized_datasets[\"test\"]['author'])):\n",
    "    # We pull the predicted label from each prediction\n",
    "    # The model is only able to predict based on the 512 first tokens\n",
    "    pred_lab = pipe(tokenized_datasets[\"test\"][idx]['article'][:512])[0]['label']\n",
    "    \n",
    "    # The model outputs strings. \n",
    "    # We pull the number from it and turn it into an integere\n",
    "    pred_lab = int(re.findall(r'\\d+', pred_lab)[0])\n",
    "    \n",
    "    # We get the real label from the test data itself\n",
    "    real_lab = tokenized_datasets[\"test\"][idx]['label']\n",
    "    \n",
    "    # Now we compare and append to a list\n",
    "    correct.append(pred_lab == real_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9eb76751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 66, True: 23})"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d275e69",
   "metadata": {},
   "source": [
    "We could do more analysis. For example:\n",
    "- Which authors did the model struggle with?\n",
    "- Which did it predict confidently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a774cc",
   "metadata": {},
   "source": [
    "Further work could include:\n",
    "- Pull out the model weights to see if there are specific words tha are important for predicting specific authors?\n",
    "- Test if we can deceive the model by performing style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8038a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
