{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e6c930",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML for Monsters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e24f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../img/darkbertscreenshot.png\" alt=\"Alternative text\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899042a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction: Goals of the Course\n",
    "\n",
    "\n",
    "I am supposed to write here about what machine learning is, what it might or might not become for all of us, with all of us behind its back, on its case, under its skin, peeking through the hedges of its enclosure. I'm supposed to speak to its affects and potentials, illuminate its pitfalls, stab at its weak points. Instead, I look out the window at the St. Lawrence. I see a swarm of small insects with papery wings dancing under a tree in front of the pale peach dying light. I think of emerging programs of collective intelligence, growing out of top-down programs for collecting intelligence. Of how we can learn from those around us, new ways of seeing, of statistics as a practice of quiet observation. I see the water, brackish and stratified by the different textures of its varied currents. The wide water of the river slowly cleaning mud so thoroughly that in three years it will have forgotten being dumped out of a sewage plant, will have been transposed by completely new mud. Mudamorphosis.\n",
    " \n",
    "The biggest question I am bringing to this course, that I am stuck on, that i hope we might stick to, stick it to, or even stick up, is: Do we fuck with machine learning? If so, how? Some of us are already fuck with it and wonder why, some of us are curious if it has anything to do with us, or to discover how exactly it already fucks us.  \n",
    "\n",
    "I've been coding statistical models of meaning a decade, wading into murky waters following the siren song of the playful machine. I'm in deep. Deep in the mud of a river where more and more industrial waste gets dumped every day. I've been drawn in, drawn on by this suspicion of potential, barely visible in the far distance, more felt than seen. But as I approach, it recedes. I study meaning space so i can walk around in it. Not as a way to map the world but as a world unto itself, to be explored, to get lost in. I'm here because I need help, and I hope that if we all come at it from different directions eventually we'll have it surrounded or even if we don't, we'll have passed through each other.\n",
    "\n",
    "Let's fuck around and find it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7e4c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three Perspectives on Machine Learning\n",
    "\n",
    "In this course, we'll use natural language processing (NLP) as a context to explore the state of machine learning and AI from many perspectives: critical and constructive, philosophical and practical. \n",
    "\n",
    "**critique / AI genealogies** : how are logics of capture and (mis)representation encoded into machine learning models, and how are these models currently employed within apparatuses of control?\n",
    "\n",
    "**combat / AI adversaries:** how can the function of ML models be exploited to get them to behave in unintended ways? How can we resist the push to be desired users?\n",
    "\n",
    "**construct / AI poetics:** can we imagine use ML in ways that step outside the paradigm of representation, prediction, and control? What does partisan machine learning look like? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadabe94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three Levels of Exploration\n",
    "\n",
    "In order to develop these perspectives, we will cover (mostly language-related) machine learning technologies at three levels: \n",
    "\n",
    "- **mathematical:** familiarity and comfort with formalisms and concepts from statistics from linear algebra and probability theory\n",
    "    - information theory and probability\n",
    "    - loss functions\n",
    "    - backpropagation\n",
    "- **operational:** ability to use tools in the ecosystem that implement these formalisms\n",
    "    - text processing\n",
    "    - language modeling (guessing the next word)\n",
    "    - classification\n",
    "- **socio-historical:** how did these foundational formalisms come to be so important in the field? how do the  commercial interests and ideologies of machine learning developers and affect the way problems are defined and approached? \n",
    "    - applications (weaponized & radical)\n",
    "    - social theory\n",
    "    \n",
    "These levels correspond to degrees of contextualization. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafd849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Synthesis\n",
    "\n",
    "We welcome any mode you wish to engage with the material. If you want to compare the effect of different kinds of loss functions using pure math or a fake procedurally generated dataset, go for it! And teach us what you learn. If your project is a prototype for an app, or a proposal for a new kind of dataset and a plan to collect it, we will stay up late with you designing the pilot. If you want to write a philosophical essay on the  impossibility of 'ML for social good', use us to workshop your ideas.\n",
    "\n",
    "We want to push aas far as we can in all of these directions. But we also insist on taking a holistic stance, and hope that you do to: the lenses we look through and the levels we look at necessarily interact.\n",
    "We believe that in order to approach ML through any of these lenses or at any of these levels, and do it well, it is absolutely necessary to engage with the others.\n",
    "\n",
    "To understand the math, it helps to understand the social contexts that led to its development, and to use tools that allow us to abstract away from the implementation details once we understand them. EXAMPLE\n",
    "\n",
    "To understand and critique the current sociocultural landscapes of AI, we have to know how to interpret the methods used by applications, at the formal and toolchain level. To intervene in this landscape through misuse of models, deception of models, or application of ML to our own curiosities and problems, we need to be comfortable using the tools. EXAMPLE\n",
    "\n",
    "To confidently apply tools to a variety of situations and new kinds of data, we need to develop firm intuitions about the underlying math.  Conversely, thinking through sociological and epistemological questions raised by different modeling techniques will enable us to recognize what possibilities the ecosystem of tools open up and what possibilities they foreclose on. EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782f33a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structure of the Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d795a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our classes will be a combination of\n",
    "\n",
    "* Interactive lectures w/ coding exercises in Jupyter notebooks\n",
    "* Slightly longer guided labs\n",
    "* 4 discussion sections throughout the week anchored by readings\n",
    "* Projects\n",
    "    - solo or group\n",
    "    - we'll get started thinking about these early\n",
    "    - ex: make a Streamlit app that queries a large language model\n",
    "    - ex: design a data collection pri\n",
    "    \n",
    "    \n",
    "Over the course of a week we'll learn all the foundational math and techniques for building our own ML pipeline, including training our own language model that we use to train a neural net classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d5f71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Critique\n",
    "\n",
    "- dissect automated content moderation by implementing it in pytorch\n",
    "\n",
    "- machine learning as pathological recapitulation of the past - Anne Dufourmantelle\n",
    "- how labeled datasets define categories and objectives\n",
    "\n",
    "- who decides what problems are interesting to work on, what problems deserve to be \"solved\", and why?\n",
    "- limits (and affordances) of quantification, encoding, problematic of information\n",
    "- empirical experimentation: how do the choices we make affect the constructions of categories and norms\n",
    "    - . e.g., how do our representations change when we use a different training corpus?\n",
    "    - what about preprocessing techniques\n",
    "    - feature selection\n",
    "- how do we measure the difference between desired and observed behaviors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050d86e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combat\n",
    "\n",
    "- how is biopolitical power exercised in language models?\n",
    "- prompt injection / prompt leaking\n",
    "- evading the censor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49005d52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Construct\n",
    "- text mashups with n-gram language models\n",
    "- formulating the problem: how can we design tasks.\n",
    "- autonomy: open source language models and ML tools\n",
    "- building LLM apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75677b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is NLP\n",
    "\n",
    "> \"Be able to solve problems that require deep understanding of text\" - Greg Durrett CS 388 NLP\n",
    "\n",
    "> \"The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\" - Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47750766",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do the people who wrote these definitions think it means to process language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61f083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is NLP\n",
    "\n",
    "\"Be able to **solve problems** that require deep understanding of text\" - Greg Durrett CS 388 NLP\n",
    "\n",
    "\"The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then **accurately extract information** and insights **contained** in the documents as well as **categorize and organize** the documents themselves.\" - Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43643c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Key concepts\n",
    "    * desired behavior (both us and the model)\n",
    "    * the meaning is in the data (conduit metaphor - Michael Reddy)\n",
    "    * language as categorization engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445456f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbolic AI\n",
    "## a.k.a. Good Old Fashioned AI (GOFAI)\n",
    "\n",
    "- rule-based logic\n",
    "- ontologies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad618b5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dartmouth Summer Research Project on Artificial Intelligence (1956)\n",
    "\n",
    "- Participants : Claude Shannon, Marvin Minsky, & others\n",
    "- Funded by Rockefeller Foundation\n",
    "- Proposal: http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf\n",
    "\n",
    "\"every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ba16b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Georgetown IBM experiment (1954)\n",
    "\n",
    "<img src=\"../img/electronicbrain.png\" alt=\"Alternative text\" />\n",
    "\n",
    "https://www.youtube.com/watch?v=aygSMgK3BEM\n",
    "    \n",
    "Russian - English \"translation\" system\n",
    "\n",
    "> \"this will be quite an adequate speed to deal with the whole output of the soviet union in just a few hours' computer time a week.\"\n",
    "\n",
    "DoD funded project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350ba55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happened?\n",
    "\n",
    "> \"the spirit is willing but the flesh is weak.\"\n",
    "\n",
    "Translate this to Russian, and then translate back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5898b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> \"the vodka is good but the meat is rotten.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca88b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ELIZA rogerian psychoanalysis ( Joseph Weizenbaum, 1966)\n",
    "\n",
    "<img src=\"../img/ELIZA_conversation.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c8aec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SHRDLU blocks world (1968-70)\n",
    "\n",
    "<img src=\"../img/shrdlu.jpg\" alt=\"Alternative text\" />\n",
    "\n",
    "Terry Winograd MIT AI lab dissertation\n",
    "\n",
    "terminal connected to a robot arm that interacted with a 'blocks world'\n",
    "\n",
    "https://hci.stanford.edu/~winograd/shrdlu/AITR-235.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27dd617",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Perceptron\n",
    "\n",
    "In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. \n",
    "\n",
    "<img src=\"../img/xor.png\" alt=\"Alternative text\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b4123",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# AI Winter\n",
    "<img src=\"../img/aiwinter.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50673f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical NLP (1990s - 2000s)\n",
    "\n",
    "- Probabilistic rules are inferred through statistical regularities in corpora\n",
    "- N-gram language models\n",
    "- Logistic regression\n",
    "- Designed features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a192d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Machine Translation (SMT)\n",
    "\n",
    "<img src=\"../img/emstep5.png\" alt=\"Alternative text\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a2035",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What's the probability that 'maison' is translated as house?\n",
    "* For example, the outcome might be: 'maison' is translated as 'house'\n",
    "* Expectation Maximization (EM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b661e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "<img src=\"../img/logistic_regression.png\" alt=\"Alternative text\" width=\"500\" />\n",
    "\n",
    "* We have access to a number of known variables or 'features'\n",
    "* learn statistical regularities between the presence of those features and the outcomes we are interested in predicting. \n",
    "* A lot of work goes into designing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076310e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features\n",
    "\n",
    "* Each axis/dimension represents some feature of the data, which might be a document, might be a person. In practice, we represent the data along way more than two dimensions.\n",
    "* The goal ist to maximize the separation of your data in geometric space, according to the divisions you thing are important. In this way the goal of our project influences what kind of **digital objects** we create. (Yuk Hui)\n",
    "* Choose characteristics that are relevant to the task (same document, different features, depending on what we are trying to do)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c560b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Drawing lines in meaning space\n",
    "> Biopower appears to function by dividing people into those who must live and those who must die. As it proceeds on the bases of a split between the living and the dead, such power defines itself in relation to the biological field---of which it takes control and in which it invests itself. This control presupposes a distribution of human species into groups, a subdivision of the population into subgroups, and the establishment of a biological caesurea between these subgroups. Foucault refers to this using the seemingly familiar term \"racism\"\n",
    "\n",
    "Necropolitics, Achille Mbembe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5c5d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributional Semantics\n",
    "\n",
    "Latent Semantic Indexing\n",
    "\n",
    "<img src=\"../img/lsipatent.png\" alt=\"Alternative text\" />\n",
    "\n",
    "Example term x document co-occurence matrix from Scott Deerwester & Susan Dumais's patent for Bell Labs, filed 1988.\n",
    "\n",
    "- Used to discover \"implicit higher order structure\"\n",
    "- Important in information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28118c2",
   "metadata": {},
   "source": [
    "## NLP Pipelines\n",
    "\n",
    "NLP systems were organized as pipelines. The idea was to first process a sentences in a way that extracts a bunch of useful structural information about it---parts of speech, parsing sentences, building semantic representations. The outputs of these 'upstream tasks' are used as input representations in other systems that perform specific operations of interest: mostly machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b94ba9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Upstream\" Tasks - Low-Level Features\n",
    "\n",
    "Syntactic Processing\n",
    "1. Tokenization\n",
    "2. Linguistic Annotation\n",
    "   - Part of Speech tagging\n",
    "   - Syntactic parsing\n",
    "\n",
    "   \n",
    "Semantic Processing\n",
    "1. Topic Modeling\n",
    "3. Semantic similarity\n",
    "4. Named Entity Recognition\n",
    "5. Relation Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468b9b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Downstream\" Tasks - Applications\n",
    "\n",
    "The main areas of applied interest in NLP during this era were things like\n",
    "\n",
    "- Machine Translation\n",
    "- Information Retrieval / document querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6eb833",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural NLP\n",
    "\n",
    "- Feature engineering no longer necessary\n",
    "- Features are learned.\n",
    "- http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf43e67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi Layer Perceptron (Bengio 2003)\n",
    "\n",
    "<img src=\"../img/bengio2003neurallanguagemodels.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b01a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning \n",
    "\n",
    "<img src=\"../img/deeplearning.png\" alt=\"Alternative text\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a52d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word2Vec (Google: Mikolov et al. 2013)\n",
    "\n",
    "<img src=\"../img/word2vecgraph.png\" alt=\"Alternative text\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5baec5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (Vaswani et al., 2017)\n",
    "\n",
    "\n",
    "Attention allows the model to decide which words to 'attend' to during the generation process: based on the words it already knows about, which words are the most important for guessing the next word?\n",
    "\n",
    "<img src=\"../img/transformers.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e1832",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BERT (google: Devlin et al., 2018)\n",
    "\n",
    "<img src=\"../img/attention_alignment.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3405fb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* enter the era of pretrained language models\n",
    "* just the encoder.\n",
    "* Pre-Train + Fine-tune Paradigm\n",
    "* fine tuning updates the model parameters on domain specific data\n",
    "* people are still training models on their specific datasets (SciBERT, DarkBERT, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439526c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GPT\n",
    "\n",
    "<img src=\"../img/astrofiziks.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08255d29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../img/zero-shot.jpg\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7f6d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Innovation... if you can call it that\n",
    "\n",
    "<img src=\"../img/transformers_bert_gpt.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465c9eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"../img/gpt2-sizes-hyperparameters-3.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24c86a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scale\n",
    "\n",
    "Basically Nothing has changed since 2018 except the addition of parameters\n",
    "\n",
    "| Model       | Training Data | Parameters |\n",
    "| ----------- | ----------- |------------|\n",
    "| GPT2        | 40G (WebText)       | 1.5 Billion |\n",
    "| GPT3        |   45 Terabytes (45K GB)      | 750 B | \n",
    "| GPT 3.5 | Above + | \n",
    "| ChatGPT | Above + Conversation Data | \n",
    "| GPT 4 | Unknown |  Eight models with 220 billion parameters each |\n",
    "\n",
    "And fine-tuning the model to be a good bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22ac59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* so it kind of feels like we're in the middle of an AI arms race\n",
    "* there's been a lot of fearmongering about what the robot overlords will do if they aren't 'aligned' with 'human' values.\n",
    "* there's also been a ton of valid critique of models perpetuating social injustices. They are racist, sexist.\n",
    "* there have been calls to regulate \"AI\", aka the development of language models, on many sides\n",
    "* unlikely given that AI is seen as a strategic technology\n",
    "    - The Age of AI and our Human Future - Henry Kissinger + Eric Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d7e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ACL 2023 Keynote - Geoffrey Hinton\n",
    "\n",
    "- 'godfather of ai' - many are saying this. His work popularized backpropagation (Rumelhart et al., 1986)\n",
    "-  \"LLMs have subjective experience\"\n",
    "\n",
    "### The Future\n",
    "* digital computers share memory; analog computers are faster (cheap transistors)\n",
    "* flocks of analog language models without shared weights. they'll teach each other, and\n",
    "* when the computer dies, the memory dies with it\n",
    "* backpropagation won't serve us anymore, we need something new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f0157",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks: Supervised and Unsupervised\n",
    "\n",
    "<img src=\"../img/supervisedunsupervised.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0db14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tasks\n",
    "\n",
    "- Start with a decision problem: given an input (sentence, document, photo?) can i sort it into the right category?\n",
    "- Build a dataset of a lot of examples of inputs with their **ground truth** labels\n",
    "- Amazon Mechanical Turk \n",
    "- This dataset comes to define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16468f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples of Tasks\n",
    "\n",
    "1. Search\n",
    "2. Question Answering\n",
    "3. Image captioning\n",
    "4. Speech Recognition\n",
    "5. Text/Document Classification\n",
    "    - sentiment analysis\n",
    "6. Machine Translation\n",
    "9. Language Modeling - generating new text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db7556",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Surveillance Capitalism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd673e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### PredPol\n",
    "\n",
    "<img src=\"../img/predpol.png\" alt=\"Alternative text\" />\n",
    "\n",
    "Recently rebranded to https://geolitica.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c2690",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Moonshot: data to end violent extremism\n",
    "\n",
    "* Partner of Google Jigsaw (https://jigsaw.google.com/the-current/)\n",
    "    - Run by Jared Cohen (counterterrorism guy; working for state department now)\n",
    "* \"Working to end online harms\" \"around the globe\"\n",
    "\n",
    "Case Study\n",
    "Location: USA\n",
    "Sector: Private\n",
    "Client: LLoyd's (the oldest continuously active insurance\n",
    "marketplace in the world)\n",
    "    \n",
    "    \"With millions of data points, we were able to design, train and test an algorithm that generates an overall and a daily risk score for locations across the US. These scores reflect the likelihood of threats of violence online translating into violence against people or property in the real world\"\n",
    "\n",
    "https://moonshotteam.com/resource/the-moonshot-threat-bulletin-june-2023-at-a-glance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5663b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Turnstile Hoppers\n",
    "<img src=\"../img/nycsubway.png\" alt=\"Alternative text\" />\n",
    "<img src=\"../img/fareevasion.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f42c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scanning inmate mail for gang affiliation\n",
    "<img src=\"../img/pigeonlycorrections.png\" alt=\"https://www.pigeonlycorrections.com/\" />\n",
    "<img src=\"../img/pigeonlypowered.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd702927",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Anything you don't say can and will be used against you\n",
    "\n",
    "<img src=\"../img/semanticreconstruction.png\" alt=\"Alternative text\" />\n",
    "<img src=\"../img/decodedstimulus.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0a389",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications: Supervised or Unsupervised?\n",
    "\n",
    "* Machine translation\n",
    "* Speech-segmentation\n",
    "* Automated content moderation\n",
    "* Auto-generated tags / topic labels\n",
    "* Smart search results - Question answering\n",
    "* Recommendation systems\n",
    "* Sentiment analysis\n",
    "* Fraud detection / loan approval\n",
    "* Language modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d50732",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semi-supervised\n",
    "\n",
    "- In between supervised and unsupervised learning\n",
    "- I have some data with gold labels\n",
    "- I have a lot of data without labels\n",
    "- Can I somehow extend insights from the labeled data by using unlabeled data?\n",
    "\n",
    "Q: Why should this work?\n",
    "Q: Assume that each gold label is a coherent group.\n",
    "- Can unlabeled data give you insight on group boundaries?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cde108",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity \n",
    "\n",
    "* Pick an industry that uses AI (hint: they all do)\n",
    "* Search for some things that they use it for (use cases)\n",
    "* Choose one application \n",
    "* Find out details about how this application is developed\n",
    "* What are the data? supervised or unsupervised? is it regulated? updated?\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
