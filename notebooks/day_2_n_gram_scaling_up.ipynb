{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd532aad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Here's our N-gram model: what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66071bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"By this liberty they entered into a very laudable emulation to do all of them \\\n",
    "what they saw did please one. If any of the gallants or ladies should say, Let us drink, \\\n",
    "they would all drink.  If any one of them said, Let us play, they all played.  If one said, \\\n",
    "Let us go a-walking into the fields they went all.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3c09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ngrams for a corpus\n",
    "def ngrams(text, n):\n",
    "    n_grams = []\n",
    "    for i in range(n-1, len(tokenized_corpus)): \n",
    "        n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "    return n_grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0792006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        \n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        # separate punctuation from previous word\n",
    "        spaced_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        \n",
    "        # split into sentences\n",
    "        sentences = spaced_corpus.split('.')\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokenized_corpus += words\n",
    "        \n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012be51",
   "metadata": {},
   "source": [
    "# Scaling up\n",
    "\n",
    "If we keep increasing n, our generated text starts to repeat our input text almost word for word. To get interesting behavior, we have to increase the size of the corpus. Let's try with a much bigger corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6702e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/gabriellachronis/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "!python3 -m nltk.downloader gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f955ed1",
   "metadata": {},
   "source": [
    "NLTK comes with several built in corpora, including a selection of books from project gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c38e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import corpus using an alias to avoid namespace confustion with our corpus variable\n",
    "from nltk import corpus as corpiss \n",
    "\n",
    "corpiss.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69afbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'King',\n",
       " 'James',\n",
       " 'Bible',\n",
       " ']',\n",
       " 'The',\n",
       " 'Old',\n",
       " 'Testament',\n",
       " 'of',\n",
       " 'the',\n",
       " 'King',\n",
       " 'James',\n",
       " 'Bible',\n",
       " 'The',\n",
       " 'First',\n",
       " 'Book',\n",
       " 'of',\n",
       " 'Moses',\n",
       " ':',\n",
       " 'Called',\n",
       " 'Genesis',\n",
       " '1',\n",
       " ':',\n",
       " '1',\n",
       " 'In',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'God',\n",
       " 'created',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " '1',\n",
       " ':',\n",
       " '2',\n",
       " 'And']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv = corpiss.gutenberg.words('bible-kjv.txt')\n",
    "list(kjv)[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cfdb2",
   "metadata": {},
   "source": [
    "### A Few final adjustments\n",
    "\n",
    "We have some housekeeping things to take care of. \n",
    "\n",
    "1. Because we have encoded sentence breaks as a string of start and stop sequences, we now will generate a lot of them in our output. We add function to strip them out, and update our generate method to strip out these tokens before printing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703885bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son . even so , father ; for the press is full , the fats overflow ; for their wickedness . 119 : 59 i thought on my ways , as a seal upon him , and would none of my words . 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz . 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab . 134 : 3 the aged women likewise , that they escaped all safe to land . spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up . unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning . 106 : 18 and samuel told him every whit , and hid snares for my feet .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_stops(string):\n",
    "    \"\"\"\n",
    "    function to convert the stop/start sequence back into periods.\n",
    "    strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "    and replaces them with a period.\n",
    "\n",
    "    then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "    string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "    string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "test_string = '<s> <s> <s> take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son </s> </s> </s> <s> <s> <s> even so , father ; for the press is full , the fats overflow ; for their wickedness </s> </s> </s> <s> <s> <s> 119 : 59 i thought on my ways , as a seal upon him , and would none of my words </s> </s> </s> <s> <s> <s> 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz </s> </s> </s> <s> <s> <s> 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab </s> </s> </s> <s> <s> <s> 134 : 3 the aged women likewise , that they escaped all safe to land </s> </s> </s> <s> <s> <s> spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up </s> </s> </s> <s> <s> <s> unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning </s> </s> </s> <s> <s> <s> 106 : 18 and samuel told him every whit , and hid snares for my feet </s> </s> </s> <s> <s> <s>'\n",
    "model = NgramModel(corpus, 3)\n",
    "add_stops(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b86ba",
   "metadata": {},
   "source": [
    "2. numbers are a problem for n-gram models becayse there are so many of them. we don't want to eliminate them, because they are meaningful, but we want to abstract away from the individual numbers. In addition, we might want to get rid of some other things like parentheticals and quotes, becayse these impossible for our model to keep track of given it's amount of memory. We can take care of these things in the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f57c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def _tokenize(self, corpus):\n",
    "    # The list of regular expressions and replacements to be applied\n",
    "    # the order here matters! these replacements will happen in order\n",
    "    replacements = [\n",
    "         [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "        ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "        ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "        ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "        ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "    ]\n",
    "\n",
    "    # This is a function that applies a single replacement from the list\n",
    "    resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "\n",
    "    # we use the resub function to applea each replacement to the entire corpus,\n",
    "    normalized_corpus = reduce(resub, replacements, corpus)\n",
    "\n",
    "\n",
    "    sentences = normalized_corpus.split('.')\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split() # split on whitespace\n",
    "        words = [word.lower() for word in words]\n",
    "        words = list(pad_both_ends(words, n=self.n))\n",
    "        tokens += words\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b11df",
   "metadata": {},
   "source": [
    "Here is a final version of our class with all the bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e46957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from functools import reduce\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "\n",
    "    def _tokenize(self, corpus):\n",
    "        # The list of regular expressions and replacements to be applied\n",
    "        # the order here matters! these replacements will happen in order\n",
    "        replacements = [\n",
    "             [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "            ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "            ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "            ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "            ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "        ]\n",
    "        \n",
    "        # This is a function that applies a single replacement from the list\n",
    "        resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "        \n",
    "        # we use the resub function to applea each replacement to the entire corpus,\n",
    "        normalized_corpus = reduce(resub, replacements, corpus)\n",
    "        \n",
    "        \n",
    "        sentences = normalized_corpus.split('.')\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokens += words\n",
    "        \n",
    "        return tokens\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "        string = add_stops(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "    \n",
    "    def add_stops(string):\n",
    "        \"\"\"\n",
    "        function to convert the stop/start sequence back into periods.\n",
    "        strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "        and replaces them with a period.\n",
    "\n",
    "        then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "        string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "        string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966c2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(corpus, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6c62143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. if any one of the gallants or ladies should say , let us play , let us play , let us drink , they all drink , let us play , let us go a walking into a very laudable emulation to do all . by this liberty they would all of the gallants or ladies should say , let us go a walking into a walking into a walking into the fields they entered into the gallants or ladies should say , let us play , they would all . by this liberty they saw did please one of them what they went all . if any of them said , they all . . if any of the gallants or ladies should say , let us go a walking into the gallants or ladies should say , they would all of them what they all of them what they entered into a very laudable emulation to do all . if any of the fields they saw did please one of them what they all of the fields they saw did please one of them said , let us play , they would all . if one said , let us drink . '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591a9ce",
   "metadata": {},
   "source": [
    "We try generating a 4-gram model with the King James Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20358",
   "metadata": {},
   "source": [
    "Our model expects its training corpus in the form of a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ee616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = (' ').join(kjv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be7089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(kjv, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2637c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ezra [number] : [number] zadok his son , take now thy son , he shall know that my redeemer liveth , and thy patience , and thy fairs , thy merchandise , and all knowledge ; [number] : [number] declare his glory among the gentiles to forsake moses , saying that he himself had dedicated , into the hand of esau : for i speak not this to condemn you : for he said to gehazi , gird up thy loins , and mourned for his son . his glory covered the heavens , jesus the son of melea , which was , and samuel was laid down to sleep ; [number] : [number] scornful men bring a city into a snare : but wise men turn away wrath . fare ye well . destroy not him with thy robe , and killed him '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1d6e2",
   "metadata": {},
   "source": [
    "# Let's do a mashup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c1a90",
   "metadata": {},
   "source": [
    "Intro to beautiful soup for scraping web text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86054ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4) (2.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "#cover t-i-the-invisible-committe-now-8.png\n",
      "#pubdate 2018-02-25 19:37:42 +0000\n",
      "#title Now\n",
      "#author The Invisible Committe\n",
      "#LISTtitle Now\n",
      "#SORTauthors comité invisible\n",
      "#date 2017\n",
      "#source Retrived on February 18, 2018 from https://illwilleditions.noblogs.org/files/2018/02/Invisible-Committee-NOW-READ.pdf\n",
      "#lang en\n",
      "#SORTtopics insurrectionary, communization\n",
      "#notes The Invisible Committee are an anonymous fragment of the Imaginary Party.\n",
      "First published as *Maintenant* in May, 2017.\n",
      "Translated by Robert Hurley.\n",
      "\n",
      "\n",
      "No more waiting.\n",
      "No more hoping.\n",
      "No more letting ourselves be distracted, unnerved.\n",
      "Break and enter.\n",
      "Put untruth back in its place.\n",
      "Believe in what we feel.\n",
      "Act accordingly.\n",
      "Force our way into the present.\n",
      "Try. Fail this time. Try again. Fail better.\n",
      "Persist. Attack. Build.\n",
      "Go down one’s road.\n",
      "Win perhaps.\n",
      "In any case, overcome.\n",
      "Live, therefore.\n",
      "Now...\n",
      "\n",
      "\n",
      "** Tomorrow Is Cancelled\n",
      "\n",
      "\n",
      "\n",
      "[[t-i-the-invisible-committe-now-1.png f]]\n",
      "\n",
      "\n",
      "\n",
      "All the reasons for making a revolution are there. Not one is lacking. The shipwreck of politics, the arrogance of the powerful, the reign of falsehood, the vulgarity of the wealthy, the cataclysms of industry, galloping misery, naked exploitation, ecological apocalypse—we are spared nothing, not even being informed about it all. “Climate: 2016 breaks a heat record,” Le Monde announces, the same as almost every year now. All the reasons are there together, but it’s not reasons that make revolutions, it’s bodies. And the bodies are in front of screens.\n",
      "\n",
      "One can watch a presidential election sink like a stone. The transformation of “the most important moment in French political life” into a big trashing fest only makes the soap opera more captivating. One couldn’t imagine Koh-Lanta with such characters, such dizzying plot twists, such cruel tests, or so general a humiliation. The spectacle of politics lives on as the spectacle of its decomposition. Disbelief goes nicely with the filthy landscape. The National Front, that political negation of\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "\n",
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/the-invisible-committe-now.muse'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "print(text[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c57d0783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4495207\n",
      "199704\n",
      "3994080\n"
     ]
    }
   ],
   "source": [
    "print(len(kjv))\n",
    "print(len(text))\n",
    "print(len(text * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f269771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mashup = text * 20 + kjv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1277f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(mashup, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a024dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those in the happiness , and under de mayerne , [number] : [number] : rottenness in which nullifies all , so that endureth continually ? watchman went into thine handmaidens came in jonathan ? why sleep the sovereignty of course was malchus . so can it on in beersheba , he be fully persuaded of a vast police officers stood among yourselves off before the hall again seven sabbaths , and iron pillar after men they may breed abundantly i chose israel is proud wrath ; and the decree which he return . [number] and it caused an unorganized chaos of life , and get him lie one or by the video of us swallow it were still are plainly of triumph . ” pierre peuchmaurd , they kept by the 1960s , and david stooped for the land of his fury of sabotage of omri king , so abijah pursued mine adversity consider ; and sheba gave him in natural branches of life of lies against any point to keep house also came the laws thereof ninety and flourishing in egypt riseth against yourselves ? of art angry man condemned , now philip was little oil within thee waste and temperament are not fall of regime in shiloh the police are come in our communications between every province , what joab did worship : and baal , behold , indicates that rule one still on by your north ; bilhan : for war ? was said , as he prosperously because in thy peace they keep under absolute fragmentation of moab said unto anger they hearkened ; [number] : pull thee about a molten sea wrought , saying abroad any importance shows rather than half mount to see rain stop the non homogeneity of killers . if he hath warned from between them well said again which stumble and move them : [number] : and pure minds were affrighted , the levites that mordecai alone to be long preaching , hold themselves are given all the hay , which cut thyself marvellous works forty nights '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363b0b1",
   "metadata": {},
   "source": [
    "Wow!\n",
    "\n",
    "'among the hills that are weaned from the waters saw thee polluted in thy glory above all people , from beersheba to mount up with . [number] : [number] and shaalabbin , and partly broken . report , that jehoshaphat the king is among us still believed in hope ; patient in spirit ; and half of thy power preserve thou those that served in the womb : if jacob take a lump of figs were set there upon him shall inherit all things thereon . in most militants this search for my gold and the lord separated the sons shall eat clean provender , which loveth thee and abishai , and kings have had dominion over our cattle . then he sacrificed also and to whomsoever he will prosper us ; thus have been occupied therein . yellowed figures of cherubims and palm trees : they serve not thy left side , upon their altars : but according to our hand be upon every fowl of the european union . all these did moses command joshua , this do ye look on us ; because a deep sleep fell upon it before saul : [number] wise men , let them turn their mourning . after theo’s rape , a strong wind ? [number] : [number] open thou mine affliction . to him remaining . rather comically , he took counsel how they might attain to innocency ? [number] : [number] for behold the place hormah  '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646db1d",
   "metadata": {},
   "source": [
    "# Exercise / Homework??\n",
    "\n",
    "Make a mashup of two texts. They can be texts you wrote (a collection of tweets, an essay), or from anywhere. You can use libgen to find books and Calibre to convert them to text. Either paste the text directly into a notebook or use a Python utility for reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15494939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150118\n",
      "199704\n"
     ]
    }
   ],
   "source": [
    "alice = (' ').join(corpiss.gutenberg.words('carroll-alice.txt'))\n",
    "now = text\n",
    "\n",
    "print(len(alice))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1125b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashups = alice + text\n",
    "model = NgramModel(mashups, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33bd1a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"his eye fell on march 2015 , still can’t picture . economy until they’ve just , schizophrenia . it’s well eh , mostly kings and falls apart from all crowded together : its prevailing mood . in reply , with zadists and still managed ? alice coming into the flurry of vagabonds , you please come and down its wine ,' continued the table and humans to twist it scatters purposes . ' what porpoise , down a theory of these days \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9cef5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "most used: \n",
    "* https://notebook.community/luketurner/ipython-notebooks/notebooks/n-gram%20tutorial\n",
    "* https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "* https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "others:\n",
    "* https://eliteai-coep.medium.com/building-n-gram-language-model-from-scratch-9a5ec206b520\n",
    "* https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
    "* http://www.pygaze.org/2016/03/how-to-code-twitter-bot/\n",
    "    - code: https://github.com/esdalmaijer/markovbot\n",
    "* https://towardsdatascience.com/implementing-a-character-level-trigram-language-model-from-scratch-in-python-27ca0e1c3c3f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
