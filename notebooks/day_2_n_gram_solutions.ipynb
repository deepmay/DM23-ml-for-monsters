{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04941e5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# N-gram Language Models\n",
    "\n",
    "\n",
    "N-gram language models are a way of predicting the next word based on n-1 preceding words.\n",
    "\n",
    "We will make our own n-gram language model from scratch, with the help of a few utilities from NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f64823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/shannon_example_b.png\" alt=\"Drawing\" style=\"height: 250px;\"/><figcaption style=\"width: 280px;\">A language consisting only of the letters A, B, C, D, and E. Each letter has a probability.</figcaption></td>\n",
    "<td> <img src=\"../img/shannon_example_c.png\" alt=\"Drawing\" style=\"height: 250px;\"/><figcaption style=\"width: 350px;\">A language consisting only of the letters A, B, and C but with conditional transition probabilities.</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32fb3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/onefish.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cc3a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/quicklyran.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">Markov model for generating a sentence of infinite possible length.</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811322d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Order\n",
    "\n",
    "The order of a Markov model is the number of previous states that the model takes into account. This model is second order, because we care about the previous two words when detexrmining the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44117a79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with a toy corpus of three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ed057f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = \"\"\"By this liberty they entered into a very laudable emulation to do all of them \\\n",
    "what they saw did please one. If any of the gallants or ladies should say, Let us drink, \\\n",
    "they would all drink.  If any one of them said, Let us play, they all played.  If one said, \\\n",
    "Let us go a-walking into the fields they went all.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd79ebb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The first step of any text-based data project is usually pre-processing the text. \n",
    "\n",
    "**Tokenization:** Separate into contexts, words.\n",
    "\n",
    "**(Normalization):** Multiple spellings, capitalization.\n",
    "\n",
    "**(Lemmatization):** Different forms of a word collapse into one.\n",
    "\n",
    "**(Stop-word removal):** Discarding common words.\n",
    "\n",
    "**(Frequency Limits):** Discard very infrequent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360fa16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise \n",
    "\n",
    "### 1: Tokenization\n",
    "\n",
    "Tokenize and the corpus and normalize it by making all the words the same case. That is, split the corpus into tokens and store them in a list. The output should look like a list of strings, e.g.\n",
    "\n",
    " ```\n",
    "       tokenized_corpus = [\"by\", \"this\", \"liberty\", ... \"all\", \".\"']\n",
    " ```\n",
    "\n",
    "You will use the `str.split()` method, the `str.lower()` or `str.upper()` methods. \n",
    "\n",
    "We also want punctuation to be its own token, which will take more than just splitting on whitespace.  If you like regexes, try building one that will separate punctuation from the preceding word, so that it registers as a separate token. You can use `re.sub()` for this. Consult the Python docs (these are your friend!) for information on these functions and to look for others that might help https://docs.python.org/3.11 . No shade for asking chatGPT to do this exercise for you, but dont ask it until you are sure youre ready to see how deep the rabbit hole goes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070d35f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcda3d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by',\n",
       " 'this',\n",
       " 'liberty',\n",
       " 'they',\n",
       " 'entered',\n",
       " 'into',\n",
       " 'a',\n",
       " 'very',\n",
       " 'laudable',\n",
       " 'emulation',\n",
       " 'to',\n",
       " 'do',\n",
       " 'all',\n",
       " 'of',\n",
       " 'them',\n",
       " 'what',\n",
       " 'they',\n",
       " 'saw',\n",
       " 'did',\n",
       " 'please',\n",
       " 'one',\n",
       " '.',\n",
       " 'if',\n",
       " 'any',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gallants',\n",
       " 'or',\n",
       " 'ladies',\n",
       " 'should',\n",
       " 'say',\n",
       " ',',\n",
       " 'let',\n",
       " 'us',\n",
       " 'drink',\n",
       " ',',\n",
       " 'they',\n",
       " 'would',\n",
       " 'all',\n",
       " 'drink',\n",
       " '.',\n",
       " 'if',\n",
       " 'any',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them',\n",
       " 'said',\n",
       " ',',\n",
       " 'let',\n",
       " 'us',\n",
       " 'play',\n",
       " ',',\n",
       " 'they',\n",
       " 'all',\n",
       " 'played',\n",
       " '.',\n",
       " 'if',\n",
       " 'one',\n",
       " 'said',\n",
       " ',',\n",
       " 'let',\n",
       " 'us',\n",
       " 'go',\n",
       " 'a-walking',\n",
       " 'into',\n",
       " 'the',\n",
       " 'fields',\n",
       " 'they',\n",
       " 'went',\n",
       " 'all',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace word+punctuation with word + space + punctuation\n",
    "# e.g. \"end.\" => \"end .\" \n",
    "# (\\w) = any word character\n",
    "# ([.,?!;:]) = any punctuation character\n",
    "# the parentheses define 'capture groups', and \\1 and \\2 refer back to the first and second capture groups\n",
    "tokenized_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "tokenized_corpus = tokenized_corpus.split()\n",
    "tokenized_corpus = [word.lower() for word in tokenized_corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cca270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get N-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a7d99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishbigrams.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\"> Bigrams for the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70191a3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishpairs.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">Bigrams for the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b4cda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Get N-grams and N-gram Frequencies\n",
    "\n",
    "Build a bigram conditional frequency distribution for each state given the last state. For this you will need to write two functions:\n",
    "\n",
    "   `ngrams(text,n)` should transform a text into a list of ngrams (n-token) spans in the text. For example, ngrams(tokenized_corpus, 2) would return something like :\n",
    "\n",
    "```\n",
    "    [('by', 'this'),\n",
    "     ('this', 'liberty'),\n",
    "     ('liberty', 'they'),\n",
    "     ('they', 'entered'),\n",
    "      ...\n",
    "    ]\n",
    "```\n",
    "\n",
    "If you are feeling stuck, try writing the function for bigrams first. That is, write it for just 2-word window size,\n",
    "\n",
    "   `ngram_frequency(ngram_list)` should read in the list of ngrams generated in the last step and construct a dictionary mapping from ngrams to integer counts, like:\n",
    "    \n",
    "```\n",
    "    {('by', 'this'): 1,\n",
    "     ('this', 'liberty'): 1,\n",
    "     ('liberty', 'they'): 1,\n",
    "      ...\n",
    "    }\n",
    "```\n",
    "\n",
    "This function should be more straightforward.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e675ef00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# get the ngrams for a corpus\n",
    "def ngrams(text, n):\n",
    "    n_grams = []\n",
    "    for i in range(n-1, len(tokenized_corpus)): \n",
    "        n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efe7085",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('by', 'this'),\n",
       " ('this', 'liberty'),\n",
       " ('liberty', 'they'),\n",
       " ('they', 'entered'),\n",
       " ('entered', 'into'),\n",
       " ('into', 'a'),\n",
       " ('a', 'very'),\n",
       " ('very', 'laudable'),\n",
       " ('laudable', 'emulation'),\n",
       " ('emulation', 'to'),\n",
       " ('to', 'do'),\n",
       " ('do', 'all'),\n",
       " ('all', 'of'),\n",
       " ('of', 'them'),\n",
       " ('them', 'what'),\n",
       " ('what', 'they'),\n",
       " ('they', 'saw'),\n",
       " ('saw', 'did'),\n",
       " ('did', 'please'),\n",
       " ('please', 'one'),\n",
       " ('one', '.'),\n",
       " ('.', 'if'),\n",
       " ('if', 'any'),\n",
       " ('any', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'gallants'),\n",
       " ('gallants', 'or'),\n",
       " ('or', 'ladies'),\n",
       " ('ladies', 'should'),\n",
       " ('should', 'say'),\n",
       " ('say', ','),\n",
       " (',', 'let'),\n",
       " ('let', 'us'),\n",
       " ('us', 'drink'),\n",
       " ('drink', ','),\n",
       " (',', 'they'),\n",
       " ('they', 'would'),\n",
       " ('would', 'all'),\n",
       " ('all', 'drink'),\n",
       " ('drink', '.'),\n",
       " ('.', 'if'),\n",
       " ('if', 'any'),\n",
       " ('any', 'one'),\n",
       " ('one', 'of'),\n",
       " ('of', 'them'),\n",
       " ('them', 'said'),\n",
       " ('said', ','),\n",
       " (',', 'let'),\n",
       " ('let', 'us'),\n",
       " ('us', 'play'),\n",
       " ('play', ','),\n",
       " (',', 'they'),\n",
       " ('they', 'all'),\n",
       " ('all', 'played'),\n",
       " ('played', '.'),\n",
       " ('.', 'if'),\n",
       " ('if', 'one'),\n",
       " ('one', 'said'),\n",
       " ('said', ','),\n",
       " (',', 'let'),\n",
       " ('let', 'us'),\n",
       " ('us', 'go'),\n",
       " ('go', 'a-walking'),\n",
       " ('a-walking', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'fields'),\n",
       " ('fields', 'they'),\n",
       " ('they', 'went'),\n",
       " ('went', 'all'),\n",
       " ('all', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2 # 2 for bigram 3 for trigram, etc\n",
    "ngram_list = ngrams(tokenized_corpus, n)\n",
    "ngram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed1760",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have generated  state/transition pairs which we formed by using a “window” to look at what the next token is in a pair. Pair are in the form\n",
    "    (current state, next word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7b0d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculate frequencies of the n-grams\n",
    "def ngram_frequency(ngram_list):\n",
    "    frequency = {}\n",
    "    for ngram in ngram_list:\n",
    "        if ngram in frequency:\n",
    "            frequency[ngram] += 1\n",
    "        else:\n",
    "            frequency[ngram] = 1\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955d37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('by', 'this'): 1,\n",
       " ('this', 'liberty'): 1,\n",
       " ('liberty', 'they'): 1,\n",
       " ('they', 'entered'): 1,\n",
       " ('entered', 'into'): 1,\n",
       " ('into', 'a'): 1,\n",
       " ('a', 'very'): 1,\n",
       " ('very', 'laudable'): 1,\n",
       " ('laudable', 'emulation'): 1,\n",
       " ('emulation', 'to'): 1,\n",
       " ('to', 'do'): 1,\n",
       " ('do', 'all'): 1,\n",
       " ('all', 'of'): 1,\n",
       " ('of', 'them'): 2,\n",
       " ('them', 'what'): 1,\n",
       " ('what', 'they'): 1,\n",
       " ('they', 'saw'): 1,\n",
       " ('saw', 'did'): 1,\n",
       " ('did', 'please'): 1,\n",
       " ('please', 'one'): 1,\n",
       " ('one', '.'): 1,\n",
       " ('.', 'if'): 3,\n",
       " ('if', 'any'): 2,\n",
       " ('any', 'of'): 1,\n",
       " ('of', 'the'): 1,\n",
       " ('the', 'gallants'): 1,\n",
       " ('gallants', 'or'): 1,\n",
       " ('or', 'ladies'): 1,\n",
       " ('ladies', 'should'): 1,\n",
       " ('should', 'say'): 1,\n",
       " ('say', ','): 1,\n",
       " (',', 'let'): 3,\n",
       " ('let', 'us'): 3,\n",
       " ('us', 'drink'): 1,\n",
       " ('drink', ','): 1,\n",
       " (',', 'they'): 2,\n",
       " ('they', 'would'): 1,\n",
       " ('would', 'all'): 1,\n",
       " ('all', 'drink'): 1,\n",
       " ('drink', '.'): 1,\n",
       " ('any', 'one'): 1,\n",
       " ('one', 'of'): 1,\n",
       " ('them', 'said'): 1,\n",
       " ('said', ','): 2,\n",
       " ('us', 'play'): 1,\n",
       " ('play', ','): 1,\n",
       " ('they', 'all'): 1,\n",
       " ('all', 'played'): 1,\n",
       " ('played', '.'): 1,\n",
       " ('if', 'one'): 1,\n",
       " ('one', 'said'): 1,\n",
       " ('us', 'go'): 1,\n",
       " ('go', 'a-walking'): 1,\n",
       " ('a-walking', 'into'): 1,\n",
       " ('into', 'the'): 1,\n",
       " ('the', 'fields'): 1,\n",
       " ('fields', 'they'): 1,\n",
       " ('they', 'went'): 1,\n",
       " ('went', 'all'): 1,\n",
       " ('all', '.'): 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_frequencies = ngram_frequency(ngram_list)\n",
    "ngram_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98580160",
   "metadata": {},
   "source": [
    "# From Frequencies to Probabilities\n",
    "\n",
    "The next step is to take our frequency counts and convert them into probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08271966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives you the raw probabilities \n",
    "def ngram_probs(n_gram_frequencies, n):\n",
    "    probs = []\n",
    "    total = len(n_gram_frequencies)\n",
    "    for ngram_value, ngram_count in ngram_frequencies.items():\n",
    "        prob = ngram_count / total\n",
    "        probs.append([ngram_value, prob])\n",
    "    return probs\n",
    "\n",
    "# with laplace smoothing\n",
    "def ngram_probs_laplace_one(ngram_frequencies, n):\n",
    "    laplace_one_probs = []\n",
    "    lessgram = ngram_frequency(ngrams(tokenized_corpus, n-1))\n",
    "    vocabulary = len(ngram_frequencies)\n",
    "    for ngram_value, ngram_count in ngram_frequencies.items():\n",
    "        try:\n",
    "            prob = (ngram_count + 1) / (lessgram[ngram_value[:-1]] + vocabulary)\n",
    "            sum_prob = ++prob\n",
    "        except KeyError:\n",
    "            prob = (ngram_count + 1) / (bigram[\"UNK\"] + vocabulary)\n",
    "            sum_prob = ++math.log(prob)\n",
    "        laplace_one_probs.append([ngram_value, prob])\n",
    "    return laplace_one_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d541918",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('by', 'this'), 0.016666666666666666],\n",
       " [('this', 'liberty'), 0.016666666666666666],\n",
       " [('liberty', 'they'), 0.016666666666666666],\n",
       " [('they', 'entered'), 0.016666666666666666],\n",
       " [('entered', 'into'), 0.016666666666666666],\n",
       " [('into', 'a'), 0.016666666666666666],\n",
       " [('a', 'very'), 0.016666666666666666],\n",
       " [('very', 'laudable'), 0.016666666666666666],\n",
       " [('laudable', 'emulation'), 0.016666666666666666],\n",
       " [('emulation', 'to'), 0.016666666666666666],\n",
       " [('to', 'do'), 0.016666666666666666],\n",
       " [('do', 'all'), 0.016666666666666666],\n",
       " [('all', 'of'), 0.016666666666666666],\n",
       " [('of', 'them'), 0.03333333333333333],\n",
       " [('them', 'what'), 0.016666666666666666],\n",
       " [('what', 'they'), 0.016666666666666666],\n",
       " [('they', 'saw'), 0.016666666666666666],\n",
       " [('saw', 'did'), 0.016666666666666666],\n",
       " [('did', 'please'), 0.016666666666666666],\n",
       " [('please', 'one'), 0.016666666666666666],\n",
       " [('one', '.'), 0.016666666666666666],\n",
       " [('.', 'if'), 0.05],\n",
       " [('if', 'any'), 0.03333333333333333],\n",
       " [('any', 'of'), 0.016666666666666666],\n",
       " [('of', 'the'), 0.016666666666666666],\n",
       " [('the', 'gallants'), 0.016666666666666666],\n",
       " [('gallants', 'or'), 0.016666666666666666],\n",
       " [('or', 'ladies'), 0.016666666666666666],\n",
       " [('ladies', 'should'), 0.016666666666666666],\n",
       " [('should', 'say'), 0.016666666666666666],\n",
       " [('say', ','), 0.016666666666666666],\n",
       " [(',', 'let'), 0.05],\n",
       " [('let', 'us'), 0.05],\n",
       " [('us', 'drink'), 0.016666666666666666],\n",
       " [('drink', ','), 0.016666666666666666],\n",
       " [(',', 'they'), 0.03333333333333333],\n",
       " [('they', 'would'), 0.016666666666666666],\n",
       " [('would', 'all'), 0.016666666666666666],\n",
       " [('all', 'drink'), 0.016666666666666666],\n",
       " [('drink', '.'), 0.016666666666666666],\n",
       " [('any', 'one'), 0.016666666666666666],\n",
       " [('one', 'of'), 0.016666666666666666],\n",
       " [('them', 'said'), 0.016666666666666666],\n",
       " [('said', ','), 0.03333333333333333],\n",
       " [('us', 'play'), 0.016666666666666666],\n",
       " [('play', ','), 0.016666666666666666],\n",
       " [('they', 'all'), 0.016666666666666666],\n",
       " [('all', 'played'), 0.016666666666666666],\n",
       " [('played', '.'), 0.016666666666666666],\n",
       " [('if', 'one'), 0.016666666666666666],\n",
       " [('one', 'said'), 0.016666666666666666],\n",
       " [('us', 'go'), 0.016666666666666666],\n",
       " [('go', 'a-walking'), 0.016666666666666666],\n",
       " [('a-walking', 'into'), 0.016666666666666666],\n",
       " [('into', 'the'), 0.016666666666666666],\n",
       " [('the', 'fields'), 0.016666666666666666],\n",
       " [('fields', 'they'), 0.016666666666666666],\n",
       " [('they', 'went'), 0.016666666666666666],\n",
       " [('went', 'all'), 0.016666666666666666],\n",
       " [('all', '.'), 0.016666666666666666]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_probs(ngram_frequencies, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df165c5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('by', 'this'), 0.03278688524590164],\n",
       " [('this', 'liberty'), 0.03278688524590164],\n",
       " [('liberty', 'they'), 0.03278688524590164],\n",
       " [('they', 'entered'), 0.03076923076923077],\n",
       " [('entered', 'into'), 0.03278688524590164],\n",
       " [('into', 'a'), 0.03225806451612903],\n",
       " [('a', 'very'), 0.03278688524590164],\n",
       " [('very', 'laudable'), 0.03278688524590164],\n",
       " [('laudable', 'emulation'), 0.03278688524590164],\n",
       " [('emulation', 'to'), 0.03278688524590164],\n",
       " [('to', 'do'), 0.03278688524590164],\n",
       " [('do', 'all'), 0.03278688524590164],\n",
       " [('all', 'of'), 0.03125],\n",
       " [('of', 'them'), 0.047619047619047616],\n",
       " [('them', 'what'), 0.03225806451612903],\n",
       " [('what', 'they'), 0.03278688524590164],\n",
       " [('they', 'saw'), 0.03076923076923077],\n",
       " [('saw', 'did'), 0.03278688524590164],\n",
       " [('did', 'please'), 0.03278688524590164],\n",
       " [('please', 'one'), 0.03278688524590164],\n",
       " [('one', '.'), 0.031746031746031744],\n",
       " [('.', 'if'), 0.0625],\n",
       " [('if', 'any'), 0.047619047619047616],\n",
       " [('any', 'of'), 0.03225806451612903],\n",
       " [('of', 'the'), 0.031746031746031744],\n",
       " [('the', 'gallants'), 0.03225806451612903],\n",
       " [('gallants', 'or'), 0.03278688524590164],\n",
       " [('or', 'ladies'), 0.03278688524590164],\n",
       " [('ladies', 'should'), 0.03278688524590164],\n",
       " [('should', 'say'), 0.03278688524590164],\n",
       " [('say', ','), 0.03278688524590164],\n",
       " [(',', 'let'), 0.06153846153846154],\n",
       " [('let', 'us'), 0.06349206349206349],\n",
       " [('us', 'drink'), 0.031746031746031744],\n",
       " [('drink', ','), 0.03225806451612903],\n",
       " [(',', 'they'), 0.046153846153846156],\n",
       " [('they', 'would'), 0.03076923076923077],\n",
       " [('would', 'all'), 0.03278688524590164],\n",
       " [('all', 'drink'), 0.03125],\n",
       " [('drink', '.'), 0.03225806451612903],\n",
       " [('any', 'one'), 0.03225806451612903],\n",
       " [('one', 'of'), 0.031746031746031744],\n",
       " [('them', 'said'), 0.03225806451612903],\n",
       " [('said', ','), 0.04838709677419355],\n",
       " [('us', 'play'), 0.031746031746031744],\n",
       " [('play', ','), 0.03278688524590164],\n",
       " [('they', 'all'), 0.03076923076923077],\n",
       " [('all', 'played'), 0.03125],\n",
       " [('played', '.'), 0.03278688524590164],\n",
       " [('if', 'one'), 0.031746031746031744],\n",
       " [('one', 'said'), 0.031746031746031744],\n",
       " [('us', 'go'), 0.031746031746031744],\n",
       " [('go', 'a-walking'), 0.03278688524590164],\n",
       " [('a-walking', 'into'), 0.03278688524590164],\n",
       " [('into', 'the'), 0.03225806451612903],\n",
       " [('the', 'fields'), 0.03225806451612903],\n",
       " [('fields', 'they'), 0.03278688524590164],\n",
       " [('they', 'went'), 0.03076923076923077],\n",
       " [('went', 'all'), 0.03278688524590164],\n",
       " [('all', '.'), 0.03125]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_probs_laplace_one(ngram_frequencies, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5874829",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We've seen the word let. what is the probability that us is the next word?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c0111",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "it's NOT .063. why is this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa97cc",
   "metadata": {},
   "source": [
    "## An aside: data structures for counting words\n",
    "\n",
    "As an aside, first a few quick words about data structures in NLTK that support us in counting words (or word groups, or pieces of syntactic structure). The first is basically a dictionary mapping words to counts, called a FreqDist. Conveniently, you can just initialize it by giving it a list of items, and it will count how often each item appears in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ee615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'b': 2, 'c': 1, 'd': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK data structures for counting stuff:\n",
    "# count individual words or other items:\n",
    "import nltk\n",
    "\n",
    "fd = nltk.FreqDist([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabe47a",
   "metadata": {},
   "source": [
    "The second data structure relevant for us today is the ConditionalFreqDist. It also has counts, but it can be used to count, for each target, how often each context word appears, or more generally, how often each word appears given some other word. Say \"a\" is a target, and \"b\" and \"c\" are context items, then a ConditionalFreqDist can be used like a two-deep dictionary, whose first-level keys are called \"conditions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f683b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 1 conditions>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for targets, count context words,\n",
    "# or in general, for one sort of items, \n",
    "# count another sort of items\n",
    "cfd = nltk.ConditionalFreqDist()\n",
    "cfd[\"a\"][\"b\"] += 1\n",
    "cfd[\"a\"][\"c\"] += 1\n",
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe8c3",
   "metadata": {},
   "source": [
    "For the \"condition\" 'a', the entry is again a FreqDist object that counts appearances of 'b' and 'c':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a210a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1, 'c': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1918be4",
   "metadata": {},
   "source": [
    "You can also initialize a ConditionalFreqDist by a list of pairs. It then counts, for each first item of the pair, how often each second item appears. In the next example, the ConditionalFreqDist will record that given \"a\", both \"b\" and \"c\" appeared once, and that given \"d\", \"e\" appeared once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a72a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 2 conditions>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist([(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")])\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe1ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1, 'c': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ab893",
   "metadata": {},
   "source": [
    "# Back to our corpus\n",
    "## Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda662e",
   "metadata": {},
   "source": [
    "We then organize these pairs by the current state. We match every state to the number of possible ways to transition out of that state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d93899",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishprobs.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa959e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('by', FreqDist({'this': 1})), ('this', FreqDist({'liberty': 1})), ('liberty', FreqDist({'they': 1})), ('they', FreqDist({'entered': 1, 'saw': 1, 'would': 1, 'all': 1, 'went': 1})), ('entered', FreqDist({'into': 1})), ('into', FreqDist({'a': 1, 'the': 1})), ('a', FreqDist({'very': 1})), ('very', FreqDist({'laudable': 1})), ('laudable', FreqDist({'emulation': 1})), ('emulation', FreqDist({'to': 1})), ('to', FreqDist({'do': 1})), ('do', FreqDist({'all': 1})), ('all', FreqDist({'of': 1, 'drink': 1, 'played': 1, '.': 1})), ('of', FreqDist({'them': 2, 'the': 1})), ('them', FreqDist({'what': 1, 'said': 1})), ('what', FreqDist({'they': 1})), ('saw', FreqDist({'did': 1})), ('did', FreqDist({'please': 1})), ('please', FreqDist({'one': 1})), ('one', FreqDist({'.': 1, 'of': 1, 'said': 1})), ('.', FreqDist({'if': 3})), ('if', FreqDist({'any': 2, 'one': 1})), ('any', FreqDist({'of': 1, 'one': 1})), ('the', FreqDist({'gallants': 1, 'fields': 1})), ('gallants', FreqDist({'or': 1})), ('or', FreqDist({'ladies': 1})), ('ladies', FreqDist({'should': 1})), ('should', FreqDist({'say': 1})), ('say', FreqDist({',': 1})), (',', FreqDist({'let': 3, 'they': 2})), ('let', FreqDist({'us': 3})), ('us', FreqDist({'drink': 1, 'play': 1, 'go': 1})), ('drink', FreqDist({',': 1, '.': 1})), ('would', FreqDist({'all': 1})), ('said', FreqDist({',': 2})), ('play', FreqDist({',': 1})), ('played', FreqDist({'.': 1})), ('go', FreqDist({'a-walking': 1})), ('a-walking', FreqDist({'into': 1})), ('fields', FreqDist({'they': 1})), ('went', FreqDist({'all': 1}))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "\n",
    "cfd = ConditionalFreqDist(ngram_list)\n",
    "cfd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca2cac6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('by', <ELEProbDist based on 1 samples>), ('this', <ELEProbDist based on 1 samples>), ('liberty', <ELEProbDist based on 1 samples>), ('they', <ELEProbDist based on 5 samples>), ('entered', <ELEProbDist based on 1 samples>), ('into', <ELEProbDist based on 2 samples>), ('a', <ELEProbDist based on 1 samples>), ('very', <ELEProbDist based on 1 samples>), ('laudable', <ELEProbDist based on 1 samples>), ('emulation', <ELEProbDist based on 1 samples>), ('to', <ELEProbDist based on 1 samples>), ('do', <ELEProbDist based on 1 samples>), ('all', <ELEProbDist based on 4 samples>), ('of', <ELEProbDist based on 3 samples>), ('them', <ELEProbDist based on 2 samples>), ('what', <ELEProbDist based on 1 samples>), ('saw', <ELEProbDist based on 1 samples>), ('did', <ELEProbDist based on 1 samples>), ('please', <ELEProbDist based on 1 samples>), ('one', <ELEProbDist based on 3 samples>), ('.', <ELEProbDist based on 3 samples>), ('if', <ELEProbDist based on 3 samples>), ('any', <ELEProbDist based on 2 samples>), ('the', <ELEProbDist based on 2 samples>), ('gallants', <ELEProbDist based on 1 samples>), ('or', <ELEProbDist based on 1 samples>), ('ladies', <ELEProbDist based on 1 samples>), ('should', <ELEProbDist based on 1 samples>), ('say', <ELEProbDist based on 1 samples>), (',', <ELEProbDist based on 5 samples>), ('let', <ELEProbDist based on 3 samples>), ('us', <ELEProbDist based on 3 samples>), ('drink', <ELEProbDist based on 2 samples>), ('would', <ELEProbDist based on 1 samples>), ('said', <ELEProbDist based on 2 samples>), ('play', <ELEProbDist based on 1 samples>), ('played', <ELEProbDist based on 1 samples>), ('go', <ELEProbDist based on 1 samples>), ('a-walking', <ELEProbDist based on 1 samples>), ('fields', <ELEProbDist based on 1 samples>), ('went', <ELEProbDist based on 1 samples>)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "\n",
    "cpd = ConditionalProbDist(cfd, ELEProbDist, 10)\n",
    "cpd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d03409",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['.', 'of', 'said'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at one of the probability distributions\n",
    "\n",
    "cpd['one'].samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c35eea9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this class comes with a generate function that makes a random sample according to the probability distirbution, \n",
    "# like rolling a weighted die\n",
    "cpd['one'].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76ad10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishmarkov.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8a6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<td> <img src=\"../img/fishmarkovweights.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd93f2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: text generation with an n-gram model\n",
    "\n",
    "Write a function called generate() that generates sentences using the model. It takes as input our bigram conditional probability distribution and a 'num_sentences' parameter. It uses the `ConditionalFreqDist.generate()` method to generate one token at a time, conditioned on the last token it generated.\n",
    "\n",
    "Questions to think about: How should each sentence start? How will you know when you are done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0948bc93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if any one of them what they saw did please one of them what they went all drink . if one . if any one .'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(cpdist, num_sentences=10, seed=None):\n",
    "    \"\"\"\n",
    "    model is an nltk ConditionalProbDist\n",
    "    length is the number of tokens we wish to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    # to add to\n",
    "    string = []\n",
    "    \n",
    "    \n",
    "    seed = cpd['.'].generate()\n",
    "    string.append(seed)\n",
    "    lessgram=seed\n",
    "    \n",
    "    \n",
    "    for i in range(num_sentences):\n",
    "    # start by sampling a word that comes after a period\n",
    "        while True:\n",
    "            next_token = cpd[lessgram].generate()\n",
    "            string.append(next_token)\n",
    "            lessgram = string[-1]\n",
    "\n",
    "            if next_token == '.':\n",
    "                break\n",
    "    return ' '.join(string)\n",
    "\n",
    "generate(cpd, num_sentences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2dab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "Let's put some of the methods together in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ab34db3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BigramModel():\n",
    "    from nltk import ConditionalFreqDist\n",
    "    from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        n = 2\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        tokenized_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        tokenized_corpus = tokenized_corpus.split()\n",
    "        tokenized_corpus = [word.lower() for word in tokenized_corpus]\n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "        cfd = ConditionalFreqDist(self._ngrams)\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, 10)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "        \n",
    "    def generate(self, num_sentences=10, seed=None):\n",
    "        string = []\n",
    "        \n",
    "        seed = self._cpd['.'].generate()\n",
    "        string.append(seed)\n",
    "        lessgram=seed\n",
    "\n",
    "        for i in range(num_sentences):\n",
    "        # start by sampling a word that comes after a period\n",
    "            while True:\n",
    "                next_token = self._cpd[lessgram].generate()\n",
    "                string.append(next_token)\n",
    "                lessgram = string[-1]\n",
    "\n",
    "                if next_token == '.':\n",
    "                    break\n",
    "        return ' '.join(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4148c9e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if one of them said , they saw did please one . if one . if one said , let us go a-walking into a very laudable emulation to do all played . if any of the gallants or ladies should say , let us play , they saw did please one said , they entered into the fields they went all played . if one of them said , let us go a-walking into the fields they would all .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigramModel(corpus)\n",
    "\n",
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adc558",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalizing the bigram model\n",
    "\n",
    "The bigram model is cool, but it still generates very unusial structures like ''they would all of the fields''. While this is beautiful, we might be after something a little closer to 'passing' as language. \n",
    "\n",
    "This can be achieved by conditioning on more than one previous word at a time. So, instead of just seeing 'of', the model maintains state for the last two tokens seen: 'all of'. This reduces the number of \n",
    "\n",
    "In fact, it would be great if we could generalize our model for arbitrary n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59325081",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Write a function that will build an n-gram conditional probability distribution.\n",
    "\n",
    "**Hint:** the ConditionalFrequencyDistribution class can also be instantiated empty like this\n",
    "\n",
    "`cfd = ConditionalFreqDist()`\n",
    "\n",
    "You can add frequencies for conditions by accessing them like a dictionary, and they'll be created automatically if they don't already exist.\n",
    "Like this: \n",
    "\n",
    "`cfd[\"a\"][\"b\"] += 1`\n",
    "\n",
    "This will add one to the frequency count for the b outcome of the a condition. Conditions don't have to be strings but can be more complex objects like tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa63f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _build_distribution(self, corpus, n):\n",
    "    # we have to transform the ngram list to be priors and possible outcomes. how do you split up a trigram? a 4gram?\n",
    "    cfd = ConditionalFreqDist()\n",
    "    for ngram in self._ngrams:\n",
    "        condition = tuple(ngram[0:n-1]) \n",
    "        outcome = ngram[n-1]\n",
    "        cfd[condition][outcome] += 1     \n",
    "    cpd = ConditionalProbDist(cfd, ELEProbDist, 10)\n",
    "    self.cpd = cpd\n",
    "    return cpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f1be6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can rewrite the BigramModel class to construct an n-gram model for an arbitrary n. The class constructor takes n as an input in addition to the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10c4b53",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self.n = n\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        tokenized_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        tokenized_corpus = tokenized_corpus.split()\n",
    "        tokenized_corpus = [word.lower() for word in tokenized_corpus]\n",
    "        \n",
    "        # add pad tokens to the corpus\n",
    "        tokenized_corpus = list(pad_both_ends(tokenized_corpus, n=2))\n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "        return n_grams\n",
    "    \n",
    "        print(n_grams)\n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "                        \n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, 10)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "        \n",
    "    def generate(self, num_tokens = 20, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram.\n",
    "        \"\"\"\n",
    "        string = []\n",
    "        \n",
    "    \n",
    "        seed = self.cpd['.'].generate()\n",
    "        string.append(seed)\n",
    "        lessgram=seed\n",
    "\n",
    "        for i in range(num_tokens):\n",
    "        # start by sampling a word that comes after a period\n",
    "            next_token = cpdist[lessgram].generate()\n",
    "            string.append(next_token)\n",
    "            lessgram = string[-1]\n",
    "            \n",
    "        return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61138967",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('<s>', 'by'), <ELEProbDist based on 1 samples>), (('by', 'this'), <ELEProbDist based on 1 samples>), (('this', 'liberty'), <ELEProbDist based on 1 samples>), (('liberty', 'they'), <ELEProbDist based on 1 samples>), (('they', 'entered'), <ELEProbDist based on 1 samples>), (('entered', 'into'), <ELEProbDist based on 1 samples>), (('into', 'a'), <ELEProbDist based on 1 samples>), (('a', 'very'), <ELEProbDist based on 1 samples>), (('very', 'laudable'), <ELEProbDist based on 1 samples>), (('laudable', 'emulation'), <ELEProbDist based on 1 samples>), (('emulation', 'to'), <ELEProbDist based on 1 samples>), (('to', 'do'), <ELEProbDist based on 1 samples>), (('do', 'all'), <ELEProbDist based on 1 samples>), (('all', 'of'), <ELEProbDist based on 1 samples>), (('of', 'them'), <ELEProbDist based on 2 samples>), (('them', 'what'), <ELEProbDist based on 1 samples>), (('what', 'they'), <ELEProbDist based on 1 samples>), (('they', 'saw'), <ELEProbDist based on 1 samples>), (('saw', 'did'), <ELEProbDist based on 1 samples>), (('did', 'please'), <ELEProbDist based on 1 samples>), (('please', 'one'), <ELEProbDist based on 1 samples>), (('one', '.'), <ELEProbDist based on 1 samples>), (('.', 'if'), <ELEProbDist based on 3 samples>), (('if', 'any'), <ELEProbDist based on 2 samples>), (('any', 'of'), <ELEProbDist based on 1 samples>), (('of', 'the'), <ELEProbDist based on 1 samples>), (('the', 'gallants'), <ELEProbDist based on 1 samples>), (('gallants', 'or'), <ELEProbDist based on 1 samples>), (('or', 'ladies'), <ELEProbDist based on 1 samples>), (('ladies', 'should'), <ELEProbDist based on 1 samples>), (('should', 'say'), <ELEProbDist based on 1 samples>), (('say', ','), <ELEProbDist based on 1 samples>), ((',', 'let'), <ELEProbDist based on 3 samples>), (('let', 'us'), <ELEProbDist based on 3 samples>), (('us', 'drink'), <ELEProbDist based on 1 samples>), (('drink', ','), <ELEProbDist based on 1 samples>), ((',', 'they'), <ELEProbDist based on 2 samples>), (('they', 'would'), <ELEProbDist based on 1 samples>), (('would', 'all'), <ELEProbDist based on 1 samples>), (('all', 'drink'), <ELEProbDist based on 1 samples>), (('drink', '.'), <ELEProbDist based on 1 samples>), (('any', 'one'), <ELEProbDist based on 1 samples>), (('one', 'of'), <ELEProbDist based on 1 samples>), (('them', 'said'), <ELEProbDist based on 1 samples>), (('said', ','), <ELEProbDist based on 2 samples>), (('us', 'play'), <ELEProbDist based on 1 samples>), (('play', ','), <ELEProbDist based on 1 samples>), (('they', 'all'), <ELEProbDist based on 1 samples>), (('all', 'played'), <ELEProbDist based on 1 samples>), (('played', '.'), <ELEProbDist based on 1 samples>), (('if', 'one'), <ELEProbDist based on 1 samples>), (('one', 'said'), <ELEProbDist based on 1 samples>), (('us', 'go'), <ELEProbDist based on 1 samples>), (('go', 'a-walking'), <ELEProbDist based on 1 samples>), (('a-walking', 'into'), <ELEProbDist based on 1 samples>), (('into', 'the'), <ELEProbDist based on 1 samples>), (('the', 'fields'), <ELEProbDist based on 1 samples>), (('fields', 'they'), <ELEProbDist based on 1 samples>), (('they', 'went'), <ELEProbDist based on 1 samples>), (('went', 'all'), <ELEProbDist based on 1 samples>), (('all', '.'), <ELEProbDist based on 1 samples>)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NgramModel(corpus, 3)\n",
    "model._cpd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "434f1078",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('<s>', 'by', 'this'), <ELEProbDist based on 1 samples>), (('by', 'this', 'liberty'), <ELEProbDist based on 1 samples>), (('this', 'liberty', 'they'), <ELEProbDist based on 1 samples>), (('liberty', 'they', 'entered'), <ELEProbDist based on 1 samples>), (('they', 'entered', 'into'), <ELEProbDist based on 1 samples>), (('entered', 'into', 'a'), <ELEProbDist based on 1 samples>), (('into', 'a', 'very'), <ELEProbDist based on 1 samples>), (('a', 'very', 'laudable'), <ELEProbDist based on 1 samples>), (('very', 'laudable', 'emulation'), <ELEProbDist based on 1 samples>), (('laudable', 'emulation', 'to'), <ELEProbDist based on 1 samples>), (('emulation', 'to', 'do'), <ELEProbDist based on 1 samples>), (('to', 'do', 'all'), <ELEProbDist based on 1 samples>), (('do', 'all', 'of'), <ELEProbDist based on 1 samples>), (('all', 'of', 'them'), <ELEProbDist based on 1 samples>), (('of', 'them', 'what'), <ELEProbDist based on 1 samples>), (('them', 'what', 'they'), <ELEProbDist based on 1 samples>), (('what', 'they', 'saw'), <ELEProbDist based on 1 samples>), (('they', 'saw', 'did'), <ELEProbDist based on 1 samples>), (('saw', 'did', 'please'), <ELEProbDist based on 1 samples>), (('did', 'please', 'one'), <ELEProbDist based on 1 samples>), (('please', 'one', '.'), <ELEProbDist based on 1 samples>), (('one', '.', 'if'), <ELEProbDist based on 1 samples>), (('.', 'if', 'any'), <ELEProbDist based on 2 samples>), (('if', 'any', 'of'), <ELEProbDist based on 1 samples>), (('any', 'of', 'the'), <ELEProbDist based on 1 samples>), (('of', 'the', 'gallants'), <ELEProbDist based on 1 samples>), (('the', 'gallants', 'or'), <ELEProbDist based on 1 samples>), (('gallants', 'or', 'ladies'), <ELEProbDist based on 1 samples>), (('or', 'ladies', 'should'), <ELEProbDist based on 1 samples>), (('ladies', 'should', 'say'), <ELEProbDist based on 1 samples>), (('should', 'say', ','), <ELEProbDist based on 1 samples>), (('say', ',', 'let'), <ELEProbDist based on 1 samples>), ((',', 'let', 'us'), <ELEProbDist based on 3 samples>), (('let', 'us', 'drink'), <ELEProbDist based on 1 samples>), (('us', 'drink', ','), <ELEProbDist based on 1 samples>), (('drink', ',', 'they'), <ELEProbDist based on 1 samples>), ((',', 'they', 'would'), <ELEProbDist based on 1 samples>), (('they', 'would', 'all'), <ELEProbDist based on 1 samples>), (('would', 'all', 'drink'), <ELEProbDist based on 1 samples>), (('all', 'drink', '.'), <ELEProbDist based on 1 samples>), (('drink', '.', 'if'), <ELEProbDist based on 1 samples>), (('if', 'any', 'one'), <ELEProbDist based on 1 samples>), (('any', 'one', 'of'), <ELEProbDist based on 1 samples>), (('one', 'of', 'them'), <ELEProbDist based on 1 samples>), (('of', 'them', 'said'), <ELEProbDist based on 1 samples>), (('them', 'said', ','), <ELEProbDist based on 1 samples>), (('said', ',', 'let'), <ELEProbDist based on 2 samples>), (('let', 'us', 'play'), <ELEProbDist based on 1 samples>), (('us', 'play', ','), <ELEProbDist based on 1 samples>), (('play', ',', 'they'), <ELEProbDist based on 1 samples>), ((',', 'they', 'all'), <ELEProbDist based on 1 samples>), (('they', 'all', 'played'), <ELEProbDist based on 1 samples>), (('all', 'played', '.'), <ELEProbDist based on 1 samples>), (('played', '.', 'if'), <ELEProbDist based on 1 samples>), (('.', 'if', 'one'), <ELEProbDist based on 1 samples>), (('if', 'one', 'said'), <ELEProbDist based on 1 samples>), (('one', 'said', ','), <ELEProbDist based on 1 samples>), (('let', 'us', 'go'), <ELEProbDist based on 1 samples>), (('us', 'go', 'a-walking'), <ELEProbDist based on 1 samples>), (('go', 'a-walking', 'into'), <ELEProbDist based on 1 samples>), (('a-walking', 'into', 'the'), <ELEProbDist based on 1 samples>), (('into', 'the', 'fields'), <ELEProbDist based on 1 samples>), (('the', 'fields', 'they'), <ELEProbDist based on 1 samples>), (('fields', 'they', 'went'), <ELEProbDist based on 1 samples>), (('they', 'went', 'all'), <ELEProbDist based on 1 samples>), (('went', 'all', '.'), <ELEProbDist based on 1 samples>)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NgramModel(corpus, 4)\n",
    "model._cpd.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b826138",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But there is a problem. The generate function won't work! We need to extend it to work for arbitrary n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be580fb",
   "metadata": {},
   "source": [
    "## Exercise (if time)\n",
    "\n",
    "Implement a generate function for our arbitrary n-gram model class. It should take as input a keyword argument `num_sentences` with the number of sentences to generate. Optionally, you can write it with a `seed` keyword argument that has an initial input to the model. If no input is given, the model can't condition on the previous n-1 grams! What should it start with? One trick for dealing with this is to take the input string and append start tokens to the front of it, so there are always enough tokens to condition on. You can use the `pad_sequence` function from NLTK for this. Otherwise, you will have to deal with the start case separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93470704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "\n",
    "def generate(self, num_sentences = 1, seed = []):\n",
    "    \"\"\"\n",
    "    There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "    If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "    If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "    We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "    We can use a unigram model! \n",
    "    Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "    And how can we \n",
    "    Either way, we need a seed condition to enter into the loop with.\n",
    "    \"\"\"\n",
    "\n",
    "    # place to put generated tokens\n",
    "    string = []\n",
    "\n",
    "    if seed:\n",
    "        string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "    else:\n",
    "        string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        next_token = tuple(string[-(self.n-1):])\n",
    "\n",
    "        # keep generating tokens as long as we havent reached the stop sequence\n",
    "        while next_token != '</s>':\n",
    "\n",
    "            # get the last n-1 tokens to condition on next\n",
    "            lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "\n",
    "            next_token = self.cpd[lessgram].generate()\n",
    "            string.append( next_token )\n",
    "\n",
    "    string = ' '.join(string)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489c750",
   "metadata": {},
   "source": [
    "# DO NOT PEEK\n",
    "\n",
    "Our entire class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1cf4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        \n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        # separate punctuation from previous word\n",
    "        spaced_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        \n",
    "        # split into sentences\n",
    "        sentences = spaced_corpus.split('.')\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokenized_corpus += words\n",
    "        \n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78bd6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <s> if one said , let us drink , they would all drink </s> </s> <s> <s> by this liberty they entered into a very laudable emulation to do all of them said , let us play , they all played </s> </s> <s> <s> by this liberty they entered into a very laudable emulation to do all of them what they saw did please one </s>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NgramModel(corpus, 3)\n",
    "\n",
    "#print(model.cpd.items())\n",
    "#print(model.cpd[('<s>', '<s>')].generate())\n",
    "\n",
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012be51",
   "metadata": {},
   "source": [
    "# Scaling up\n",
    "\n",
    "If we keep increasing n, our generated text starts to repeat our input text almost word for word. To get interesting behavior, we have to increase the size of the corpus. Let's try with a much bigger corpus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f955ed1",
   "metadata": {},
   "source": [
    "NLTK comes with several built in corpora, including a selection of books from project gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3c38e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/gabriellachronis/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 -m nltk.downloader gutenberg\n",
    "# import corpus using an alias to avoid namespace confustion with our corpus variable\n",
    "from nltk import corpus as corpiss \n",
    "\n",
    "corpiss.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69afbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = corpiss.gutenberg.words('bible-kjv.txt')\n",
    "list(kjv)[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276aeb4",
   "metadata": {},
   "source": [
    "Our model expects its training corpus in the form of a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79350c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = (' ').join(kjv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff180d34",
   "metadata": {},
   "source": [
    "We try generating a 4-gram model with the King James Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80078c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(kjv, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cfdb2",
   "metadata": {},
   "source": [
    "### A Few final adjustments\n",
    "\n",
    "We have some housekeeping things to take care of. \n",
    "\n",
    "1. Because we have encoded sentence breaks as a string of start and stop sequences, we now will generate a lot of them in our output. We add function to strip them out, and update our generate method to strip out these tokens before printing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stops(string):\n",
    "    \"\"\"\n",
    "    function to convert the stop/start sequence back into periods.\n",
    "    strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "    and replaces them with a period.\n",
    "\n",
    "    then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "    string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "    string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "test_string = '<s> <s> <s> take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son </s> </s> </s> <s> <s> <s> even so , father ; for the press is full , the fats overflow ; for their wickedness </s> </s> </s> <s> <s> <s> 119 : 59 i thought on my ways , as a seal upon him , and would none of my words </s> </s> </s> <s> <s> <s> 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz </s> </s> </s> <s> <s> <s> 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab </s> </s> </s> <s> <s> <s> 134 : 3 the aged women likewise , that they escaped all safe to land </s> </s> </s> <s> <s> <s> spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up </s> </s> </s> <s> <s> <s> unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning </s> </s> </s> <s> <s> <s> 106 : 18 and samuel told him every whit , and hid snares for my feet </s> </s> </s> <s> <s> <s>'\n",
    "model = NgramModel(corpus, 3)\n",
    "add_stops(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b86ba",
   "metadata": {},
   "source": [
    "2. numbers are a problem for n-gram models becayse there are so many of them. we don't want to eliminate them, because they are meaningful, but we want to abstract away from the individual numbers. In addition, we might want to get rid of some other things like parentheticals and quotes, becayse these impossible for our model to keep track of given it's amount of memory. We can take care of these things in the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def _tokenize(self, corpus):\n",
    "    # The list of regular expressions and replacements to be applied\n",
    "    # the order here matters! these replacements will happen in order\n",
    "    replacements = [\n",
    "         [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "        ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "        ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "        ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "        ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "    ]\n",
    "\n",
    "    # This is a function that applies a single replacement from the list\n",
    "    resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "\n",
    "    # we use the resub function to applea each replacement to the entire corpus,\n",
    "    normalized_corpus = reduce(resub, replacements, corpus)\n",
    "\n",
    "\n",
    "    sentences = normalized_corpus.split('.')\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split() # split on whitespace\n",
    "        words = [word.lower() for word in words]\n",
    "        words = list(pad_both_ends(words, n=self.n))\n",
    "        tokens += words\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b11df",
   "metadata": {},
   "source": [
    "Here is a final version of our class with all the bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from functools import reduce\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "\n",
    "    def _tokenize(self, corpus):\n",
    "        # The list of regular expressions and replacements to be applied\n",
    "        # the order here matters! these replacements will happen in order\n",
    "        replacements = [\n",
    "             [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "            ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "            ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "            ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "            ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "        ]\n",
    "        \n",
    "        # This is a function that applies a single replacement from the list\n",
    "        resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "        \n",
    "        # we use the resub function to applea each replacement to the entire corpus,\n",
    "        normalized_corpus = reduce(resub, replacements, corpus)\n",
    "        \n",
    "        \n",
    "        sentences = normalized_corpus.split('.')\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokens += words\n",
    "        \n",
    "        return tokens\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "        string = add_stops(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "    \n",
    "    def add_stops(string):\n",
    "        \"\"\"\n",
    "        function to convert the stop/start sequence back into periods.\n",
    "        strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "        and replaces them with a period.\n",
    "\n",
    "        then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "        string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "        string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(corpus, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c62143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1d6e2",
   "metadata": {},
   "source": [
    "# Let's do a mashup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c1a90",
   "metadata": {},
   "source": [
    "Intro to beautiful soup for scraping web text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86054ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "\n",
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/the-invisible-committe-now.muse'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "print(text[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kjv))\n",
    "print(len(text))\n",
    "print(len(text * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mashup = text * 20 + kjv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(mashup, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a024dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363b0b1",
   "metadata": {},
   "source": [
    "Wow!\n",
    "\n",
    "'among the hills that are weaned from the waters saw thee polluted in thy glory above all people , from beersheba to mount up with . [number] : [number] and shaalabbin , and partly broken . report , that jehoshaphat the king is among us still believed in hope ; patient in spirit ; and half of thy power preserve thou those that served in the womb : if jacob take a lump of figs were set there upon him shall inherit all things thereon . in most militants this search for my gold and the lord separated the sons shall eat clean provender , which loveth thee and abishai , and kings have had dominion over our cattle . then he sacrificed also and to whomsoever he will prosper us ; thus have been occupied therein . yellowed figures of cherubims and palm trees : they serve not thy left side , upon their altars : but according to our hand be upon every fowl of the european union . all these did moses command joshua , this do ye look on us ; because a deep sleep fell upon it before saul : [number] wise men , let them turn their mourning . after theo’s rape , a strong wind ? [number] : [number] open thou mine affliction . to him remaining . rather comically , he took counsel how they might attain to innocency ? [number] : [number] for behold the place hormah  '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646db1d",
   "metadata": {},
   "source": [
    "# Exercise / Homework??\n",
    "\n",
    "Make a mashup of two texts. They can be texts you wrote (a collection of tweets, an essay), or from anywhere. You can use libgen to find books and Calibre to convert them to text. Either paste the text directly into a notebook or use a Python utility for reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15494939",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = (' ').join(corpiss.gutenberg.words('carroll-alice.txt'))\n",
    "now = text\n",
    "\n",
    "print(len(alice))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1125b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashups = alice + text\n",
    "model = NgramModel(mashups, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9cef5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "most used: \n",
    "* https://notebook.community/luketurner/ipython-notebooks/notebooks/n-gram%20tutorial\n",
    "* https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "* https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "others:\n",
    "* https://eliteai-coep.medium.com/building-n-gram-language-model-from-scratch-9a5ec206b520\n",
    "* https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
    "* http://www.pygaze.org/2016/03/how-to-code-twitter-bot/\n",
    "    - code: https://github.com/esdalmaijer/markovbot\n",
    "* https://towardsdatascience.com/implementing-a-character-level-trigram-language-model-from-scratch-in-python-27ca0e1c3c3f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
